{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data using Pushift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used pmaw wrapper in python to collect the data with Pushift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have collected the posts for the last month to identify the most engaginf subreddits (using MapReduce) and then based on these subreddits we have collected the comments dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = int(dt.datetime(2021,4,1,0,0).timestamp())\n",
    "after = int(dt.datetime(2021,3,1,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "res = []\n",
    "\n",
    "for subreddit in unique_subreddits:\n",
    "    res.append(subreddit)\n",
    "    print('Number of visited Subreddits is', len(res))\n",
    "    submissions = api.search_submissions(subreddit=subreddit, limit=None, before=before, after=after)\n",
    "    print(f'Retrieved {len(submissions)} comments from Pushshift')\n",
    "    submissions_df = pd.DataFrame(submissions)\n",
    "    test_df = test_df.append(submissions_df)\n",
    "print(f'Shape of Final DF is: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./submissions.csv', header=True, index=False, columns=list(test_df.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large size of submissions.csv (5GB) and the original dataset from Kaggle (15GB) we were using pandas read in chuncks function to analyze the structure of both datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv('submissions.csv', chunksize=10):\n",
    "    # do things with chunk\n",
    "    chunk.to_csv('./part.csv', header=True, index=False, columns=list(chunk.axes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of identified top engaging subreddits is\n",
    "\n",
    "['AskReddit', 'wallstreetbets', 'teenagers', 'news', 'funny', 'relationship_advice', 'unpopularopinion' 'CryptoCurrency', 'pokemon', 'aww'] \n",
    "\n",
    "\n",
    "For each of these subreddits we have collected the comments for the last 6 months. Then we shuffled the dataframe, so that in future while splitting into training and testing sets to have equal distribution of each subreddit in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = int(dt.datetime(2021,4,1,0,0).timestamp())\n",
    "after = int(dt.datetime(2020,10,1,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'AskReddit'\n",
    "\n",
    "comments = api.search_comments(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(comments)} comments from Pushshift')\n",
    "comments_df = pd.DataFrame(comments)\n",
    "test_df = comments_df\n",
    "print(f'Shape of Final DF is: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "shuffled = data.sample(frac = 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled.to_csv('./shuffled_comments.csv', header = True, index = False, columns = list(shuffled.axes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "data = pd.read_csv('shuffled_comments.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(data):\n",
    "    \n",
    "    data = data.drop(['all_awardings', 'associated_award', 'author_flair_background_color', 'author_flair_css_class',\n",
    "                     'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color',\n",
    "                     'author_flair_type', 'author_fullname', 'author_patreon_flair', 'awarders', \n",
    "                     'collapsed_because_crowd_control', 'comment_type', 'gildings', 'is_submitter', 'locked',\n",
    "                     'permalink', 'retrieved_on', 'send_replies', 'subreddit_id', 'top_awarded_type', 'total_awards_received', \n",
    "                     'treatment_tags', 'author_cakeday', 'distinguished', 'edited', 'media_metadata'], axis=1)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    data['created_loc_time'] = pd.to_datetime(data['created_utc'], unit='s')\n",
    "    \n",
    "    data['hour'] = [d.hour for d in data['created_loc_time']]\n",
    "    data['dayofweek'] = [d.isoweekday() for d in data['created_loc_time']]\n",
    "    data['day'] = [d.day for d in data['created_loc_time']]\n",
    "    data['month'] = [d.month for d in data['created_loc_time']]\n",
    "    data['year'] = [d.year for d in data['created_loc_time']]\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def sentiment_class_definition(comment):\n",
    "    \n",
    "        metrics = {}\n",
    "        def remove_int(text):\n",
    "            return ''.join([str(i) for i in text])\n",
    "    \n",
    "        ss = sid.polarity_scores(remove_int(comment))\n",
    "        for k in sorted(ss):\n",
    "            metrics[k] = ss[k]\n",
    "            # Divides the Body into Sentiment Classes : {1: 'HP', 2: 'MP', 3: 'N', 4: 'MN', 5: 'HN'}\n",
    "            if(metrics['compound'] > 0.6):\n",
    "                sentiment_class = 1\n",
    "            elif(metrics['compound'] > 0.25):\n",
    "                sentiment_class = 2\n",
    "            elif(metrics['compound'] > -0.25):\n",
    "                sentiment_class = 3\n",
    "            elif(metrics['compound'] > -0.6):\n",
    "                sentiment_class = 4\n",
    "            else:\n",
    "                sentiment_class = 5\n",
    "        return sentiment_class\n",
    "\n",
    "    lis = []\n",
    "    for index, row in data.iterrows():\n",
    "        lis.append(sentiment_class_definition(row['body']))\n",
    "        data['sentiment_class'] = pd.DataFrame(lis)\n",
    "        \n",
    "    data = data.drop('body', axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "Preprocessing(data)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
